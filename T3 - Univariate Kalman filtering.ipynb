{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qO- https://raw.githubusercontent.com/nansencenter/DA-tutorials/master/resources/colab_bootstrap.sh | bash -s\n",
    "from resources.workspace import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truth (or \"true state\", or \"nature\"), $x_k$, \"evolves in time (indexed by $k$)\" according to some linear \"dynamical\" model:\n",
    "$$ x_{k} = \\mathscr{M}_{k-1} x_{k-1} + q_{k-1} \\, , \\qquad (\\text{Dyn}) \\, .$$\n",
    "where $q_k$ is a random noise (process) that accounts for model errors.\n",
    "\n",
    "Here, $\\mathscr{M}_{k-1}$ is just a number. In later tutorials it will be generalized to matrices, and eventually nonlinear operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The (univariate) Kalman filter (KF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KF recursively estimates $x_k$ in successive cycles which consist of two steps :\n",
    "\n",
    "####  Exc 3.1: The forecast step: \n",
    "It is supposed that $\\;\\;\\;\\quad x_{k-1} \\sim \\mathcal{N}(\\hat{x}_{k-1}, P_{k-1})$,  \n",
    "and that (independently) $q_{k-1} \\sim \\mathcal{N}(0, Q)$.  \n",
    "\n",
    "Show that the mean and variance (i.e. the expected value and the variance) of $x_k$, respectively, are given by:  \n",
    "$b_k = \\mathscr{M}_{k-1} \\hat{x}_{k-1}$,     \n",
    "$B_k = \\mathscr{M}_{k-1}^2 P_{k-1} + Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('RV sums')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that [the sum of two Gaussian random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables#Proof_using_convolutions)  is again a Gaussian.  \n",
    "Thus, the forecast step just requires computing the new moments (mean and variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The analysis step\n",
    "\"updates\" the prior (forecast), $\\mathcal{N}(x_k \\mid \\; b_k,\\; B_k)$,  \n",
    "based on the likelihood, $\\quad\\;\\;\\;\\, \\mathcal{N}(y_k \\mid \\, x_k, \\; R)$,  \n",
    "into the posterior (analysis), $\\; \\; \\, \\mathcal{N}(x_k \\mid \\; \\hat{x}_{k}, \\, P_{k})$.  \n",
    "The update formulae was derived as the Gaussian-Gaussian Bayes' rule in [the previous tutorial](T2%20-%20Bayesian%20inference.ipynb#Gaussian-Gaussian-Bayes).\n",
    "\n",
    "This completes the cycle, which can then restart with the forecast from $k$ to $k+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A straight-line example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many mathematical methods are tagged as \"least squares\". They typically have one thing in common: some sum of squared terms is being minimized. Both (least-squares) linear regression and Kalman filtering (KF) may be derived from \"least squares\".\n",
    "Do they yield the same estimate (when applied to the linear regression problem)? That's what we'll investigate...\n",
    "\n",
    "Consider the straight line\n",
    "$$x_k = a k \\, ,     \\qquad \\qquad (1) $$\n",
    "and suppose we have observations ($y$) of the line, but corrupted by noise ($r$):\n",
    "\\begin{align*}\n",
    "y_k &= x_k + r_k \\, , \\;\\; \\qquad (2)\n",
    "\\end{align*}\n",
    "where $r_k \\sim \\mathcal{N}(0, R)$ for some $R>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up an experiment based on eqns. (1) and (2).  \n",
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.4  # Slope (xx[k] = a*k) paramterer. Unknown to be estimated.\n",
    "K = 10   # Length of experiment (final time index)\n",
    "R = 1    # Observation noise strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following simulates a series of the truth ($x$) and observations ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the naming convention: xx and yy hold time series of x and y.\n",
    "xx = np.zeros(K+1) # truth states\n",
    "yy = np.zeros(K+1) # obs\n",
    "\n",
    "for k in 1+arange(K):\n",
    "    xx[k] = a*k\n",
    "    yy[k] = xx[k] + sqrt(R)*randn()\n",
    "\n",
    "# The obs at k==0 should not be used (since we know xx[0]==0, it is worthless).\n",
    "yy[0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esitmation by linear regression\n",
    "(Least-squares) linear regression minimizes\n",
    "$$J_K(\\hat{a}) = \\sum_{k=1}^K (y_k - \\hat{a} k)^2 \\, ,  \\qquad (4)$$\n",
    "yielding the estimator\n",
    "$$\\hat{a} = \\frac{\\sum_{k=1}^K {k} y_{k}}{\\sum_{k=1}^K {k}^2} \\, . \\qquad \\qquad (6)$$\n",
    "\n",
    "**Exc 3.2:** Derive (6) from (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('LinReg deriv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.4:** Program the linear regresson estimator (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(k):\n",
    "    \"Liner regression estimator based on observations y_1, ..., y_k.\"\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('LinReg_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(k=IntSlider(min=1, max=K))\n",
    "def plot_experiment(k):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    kk = arange(k+1)\n",
    "    plt.plot(kk,xx[kk]        ,'k' ,label='true state ($x$)')\n",
    "    plt.plot(kk,yy[kk]        ,'k*',label='noisy obs ($y$)')\n",
    "    plt.plot(kk,kk*lin_reg(k) ,'r' ,label='Linear regress.')\n",
    "\n",
    "    ### Uncomment this block AFTER doing the Exc 3.8 ###\n",
    "    # pw_bb, pw_xxhat = weave_fa(bb,xxhat)\n",
    "    # pw_kf, pw_ka    = weave_fa(arange(K+1))    \n",
    "    # plt.plot(pw_kf[:3*k],pw_bb[:3*k]     ,'c'  ,label='KF forecasts')\n",
    "    # plt.plot(pw_ka[:3*k],pw_xxhat[:3*k]  ,'b'  ,label='KF analyses')\n",
    "    # #plt.plot(kk,kk*xxhat[k]/k           ,'g--',label='KF extrapolated')\n",
    "\n",
    "    plt.xlim([0,1.01*K])\n",
    "    plt.ylim([-1,1.2*a*K])\n",
    "    plt.xlabel('time index (k)')\n",
    "    plt.ylabel('$x$, $y$, and $\\hat{x}$')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Estimation by the KF\n",
    "In the following we tacke the same problem, but using the KF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations equation (2)\n",
    "yields the likelihood $p(y_k|x_k) = \\mathcal{N}(y_k \\mid x_k,R)$.  \n",
    "Hopefully this is intuitive; otherwise, a more detailed derivation will be given in tutorial 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6:** For the (univariate) KF,\n",
    "we need to reformulate the problem of estimating the parameter $a$ \n",
    "as the problem of estimating $x_k$ (yielding $\\hat{a}_k = \\hat{x}_k / k$).\n",
    "Derive and implement the \"forecast model\" $\\mathscr{M}_k$ (a function of $k>0$ only) such that\n",
    "the recursion $x_{k+1} = \\mathscr{M}_k x_k$ is equivalent to eqn (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mod(k):\n",
    "    return ### INSERT ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('Sequential 2 Recusive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.7:** Explain in which sense the KF is optimal for a larger class of problems than linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('LinReg âŠ‚ KF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So although the KF (and its implementation below) may seem like \"overkill\" for this problem,\n",
    "this \"heavy machinery\" can do a lot more, and will pay off later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.8:** Implement the KF (described at the top of this notebook) in the below code block to estimate $x_k$ for $k=1,\\ldots, K$.  \n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>NB:</b> for this example, do not use the \"Kalman gain\" form of the analysis update.\n",
    "This problem involves the peculiar, unrealistic situation of infinities\n",
    "(related to \"improper priors\") at `k==1`, yielding platform-dependent behaviour.\n",
    "These peculariaties are of mainly of academic interest, and is not our focus here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 0 # Dynamical model noise strength\n",
    "\n",
    "# Allocation\n",
    "bb    = np.zeros(K+1) # mean estimates -- prior/forecast values\n",
    "xxhat = np.zeros(K+1) # mean estimates -- post./analysis values\n",
    "BB    = np.zeros(K+1) # var  estimates -- prior/forecast values\n",
    "PP    = np.zeros(K+1) # var  estimates -- post./analysis values\n",
    "\n",
    "def KF(k):\n",
    "    \"Cycle k of the Kalman filter\"\n",
    "    # Forecast\n",
    "    if k==1:\n",
    "        BB[k] = np.inf # The \"initial\" prior uncertainty is infinite...\n",
    "        bb[k] = 0      # ... thus the corresponding mean is inconsequential.\n",
    "    else:\n",
    "        BB[k] = ### INSERT ANSWER HERE ###\n",
    "        bb[k] = ### INSERT ANSWER HERE ###\n",
    "    # Analysis\n",
    "    PP[k]    = ### INSERT ANSWER HERE ###\n",
    "    xxhat[k] = ### INSERT ANSWER HERE ###\n",
    "\n",
    "# Run estimations/computations\n",
    "for k in 1+arange(K):\n",
    "    KF(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('KF_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.10:** Go back to the animation above and uncomment the block that plots the KF estimates.  \n",
    "Visually: what is the relationship between the estimates provided by the KF and by linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('LinReg compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Exercises marked with an asterisk (*) are optional.</em>\n",
    "\n",
    "**Exc 3.12*:** This excercise proves (on paper) the result of the previous excercise.  \n",
    "Note that the KF forecast step (here with $Q=0$) can be inserted in the analysis step, forming a single couple of recursions, which are recursive:\n",
    "\n",
    " * $\\hat{x}_k = P_k \\big(y_k/R \\;+\\; \\mathscr{M}_{k-1} \\hat{x}_{k-1} / [\\mathscr{M}_{k-1}^2 P_{k-1}] \\big) \\qquad (11)$  \n",
    " * $P_k = 1/\\big(1/R \\;+\\; 1/[\\mathscr{M}_{k-1}^2 P_{k-1}]\\big) \\qquad \\qquad \\quad (12)$\n",
    "\n",
    "Now:\n",
    "\n",
    "* (a). First show that $P_K = R\\frac{K^2}{\\sum_{k=1}^K k^2} \\, . \\;\\;\\; \\qquad \\quad (13)$\n",
    "* (b). Then show that $\\hat{x}_K = K\\frac{\\sum_{k=1}^K k y_k}{\\sum_{k=1}^K k^2} = K \\hat{a}_K$, where $\\hat{a}_K$ is given by eqn (11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('x_KF == x_LinReg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.14:\n",
    "Let $x_{k+1} = \\mathscr{M} x_k$, *for a generic $\\mathscr{M}>1$ (note: $Q=0$)*. What does the sequence of $P_k$ converge to?  \n",
    "You must start from eqn (12), because eqn (13) is for the straight-line example only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('Asymptotic P when M>1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.15:\n",
    "Redo Exc 3.14, but assuming  \n",
    " * (a) $\\mathscr{M} = 1$.\n",
    " * (b) $\\mathscr{M} < 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('Asymptotic P when M=1')\n",
    "#show_answer('Asymptotic P when M<1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if $\\mathscr{M}>1$, the KF does not converge to 0 error. This is because, even though you keep gaining more information, this is balanced by the growth in uncertainty by the forecast. On the other hand, if $\\mathscr{M} \\leq 1$ then the error converges to zero.\n",
    "\n",
    "In general, however, $\\mathscr{M}$ depends on $k$, as will the other system matrices ($Q, R$).\n",
    "Then, there is no limit value that the state distribution (and its parameters) converges to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.18*:** Set $Q$ to 1 or more. Re-compute the KF estimates. By interpreting the meaning of $Q$, explain why the KF estimate is now closer to the obs (always at the latest time instance) than the linear regression estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.20*:** Now change $R$ (but don't re-run the simulation of the truth and obs). The KF estimates should not change (in this particular example). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The KF consists of two steps:\n",
    " * Forecast\n",
    " * Analysis\n",
    " \n",
    "In each step, the mean and variance must be updated. \n",
    "\n",
    "As an example, we saw that the linear regression estimate is reproduced by the KF, although it is a bit tricky to initialize the KF with infinite uncertainty. However, the KF (i.e. state estimation) is much more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Multivariate Kalman](T4%20-%20Multivariate%20Kalman.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
